{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd734a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LSTM, Embedding, Bidirectional, Normalization\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977d2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def download_resource(resource):\n",
    "    url_dl_pattern = 'http://ctdbase.org/reports/{resource}.csv.gz'\n",
    "    url = url_dl_pattern.format(resource=resource)\n",
    "    \n",
    "    print('downloading: {0}'.format(resource))\n",
    "    local_filename = 'zipped_data/' + url.split('/')[-1]\n",
    "    unzipped_filename = 'unzipped_data/' + url.split('/')[-1].replace('.gz', '')\n",
    "    \n",
    "    if os.path.isfile(unzipped_filename):\n",
    "        print('data already exists')\n",
    "        return \n",
    "\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "\n",
    "    with gzip.open(local_filename, 'rb') as f_in:\n",
    "        with open(unzipped_filename, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82211c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading: CTD_chem_gene_ixns\n",
      "data already exists\n",
      "downloading: CTD_chemicals_diseases\n",
      "data already exists\n",
      "downloading: CTD_chem_pathways_enriched\n",
      "data already exists\n",
      "downloading: CTD_genes_diseases\n",
      "data already exists\n",
      "downloading: CTD_genes_pathways\n",
      "data already exists\n",
      "downloading: CTD_diseases_pathways\n",
      "data already exists\n",
      "downloading: CTD_pheno_term_ixns\n",
      "data already exists\n",
      "downloading: CTD_exposure_studies\n",
      "data already exists\n",
      "downloading: CTD_chemicals\n",
      "data already exists\n",
      "downloading: CTD_diseases\n",
      "data already exists\n",
      "downloading: CTD_genes\n",
      "data already exists\n"
     ]
    }
   ],
   "source": [
    "resources = ['CTD_chem_gene_ixns',\n",
    "#     'CTD_chem_gene_ixn_types',\n",
    "    'CTD_chemicals_diseases',\n",
    "    'CTD_chem_pathways_enriched',\n",
    "    'CTD_genes_diseases',\n",
    "    'CTD_genes_pathways',\n",
    "    'CTD_diseases_pathways',\n",
    "    'CTD_pheno_term_ixns',\n",
    "    'CTD_exposure_studies',\n",
    "    'CTD_chemicals',\n",
    "    'CTD_diseases',\n",
    "    'CTD_genes'\n",
    "]\n",
    "\n",
    "for res in resources:\n",
    "    download_resource(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c644a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(resource):\n",
    "\n",
    "    line_number = 27\n",
    "    the_file = 'unzipped_data/{resource}.csv'.format(resource=resource)\n",
    "    with open(the_file, 'r') as reader:\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == line_number:\n",
    "                header = row.replace('# ', '').split(',')\n",
    "\n",
    "    # print(header)\n",
    "    df = pd.read_csv(the_file, skiprows=29, names=header)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee68d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_df = get_df('CTD_diseases')\n",
    "disease_df[:4]\n",
    "disease_df['ParentIDs'].str.split('|').explode()\n",
    "\n",
    "hierarchy_df = disease_df.assign(ParentIDs=disease_df['ParentIDs'].str.split('|')).explode('ParentIDs')\n",
    "\n",
    "top_of_tree = 'MESH:D019636' # neurodegenerative diseases\n",
    "level_one = hierarchy_df.loc[hierarchy_df['ParentIDs'] == top_of_tree]\n",
    "level_two = hierarchy_df.loc[hierarchy_df['ParentIDs'].isin(level_one['DiseaseID'])]\n",
    "level_three = hierarchy_df.loc[hierarchy_df['ParentIDs'].isin(level_two['DiseaseID'])]\n",
    "\n",
    "all_diseases = list(level_one['DiseaseID'].unique()) \\\n",
    "    + list(level_two['DiseaseID'].unique() ) \\\n",
    "    + list(level_three['DiseaseID'].unique() ) \\\n",
    "\n",
    "# all_diseases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64185b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the network data for visualization\n",
    "df = get_df('CTD_chemicals_diseases')\n",
    "\n",
    "park_disease_df = df.loc[df['DiseaseName'] == 'Parkinson Disease']\n",
    "park_disease_df = park_disease_df.loc[park_disease_df['ChemicalName'] == 'Dopamine']\n",
    "park_disease_df = park_disease_df.loc[park_disease_df['InferenceGeneSymbol'].notnull()]\n",
    "\n",
    "park_gene_chem_network = park_disease_df[['InferenceGeneSymbol', 'ChemicalName']]\n",
    "\n",
    "\n",
    "disease_df = park_gene_chem_network.copy()\n",
    "disease_df['DiseaseName'] = 'Parkinson Disease'\n",
    "disease_df = disease_df[['InferenceGeneSymbol', 'DiseaseName']]\n",
    "disease_df.columns = ['FROM', 'TO']\n",
    "\n",
    "\n",
    "park_gene_chem_network.columns = ['FROM', 'TO']\n",
    "network_df = pd.concat([park_gene_chem_network, disease_df])\n",
    "\n",
    "network_df.to_csv('parkinsons_network.csv', sep = '|', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5463ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disease_inference_df(disease_name=None):\n",
    "    \"\"\"\n",
    "    this will join the direct evidence to the gene network.\n",
    "    \"\"\"\n",
    "    df = get_df('CTD_chemicals_diseases')\n",
    "    # print(df[:5])\n",
    "#     if disease_name:\n",
    "#         df = df.loc[df['DiseaseName'] == 'Parkinson Disease']\n",
    "        \n",
    "    gene_df = df.loc[df['DirectEvidence'].isnull()][['ChemicalName', 'DiseaseName', 'InferenceGeneSymbol', 'InferenceScore', 'DiseaseID']]\n",
    "\n",
    "    evidence_df = df.loc[df['DirectEvidence'].notnull()][['ChemicalName', 'DiseaseName', 'DirectEvidence', 'DiseaseID']]\n",
    "    merged_df = gene_df.merge(evidence_df, on=['ChemicalName', 'DiseaseName', 'DiseaseID'])\n",
    "\n",
    "    dummy_df = pd.get_dummies(merged_df, prefix='', prefix_sep='',columns=['InferenceGeneSymbol'])\n",
    "    gb_df = dummy_df.groupby(['DiseaseName', 'ChemicalName', 'DiseaseID']).agg({np.max}).reset_index()\n",
    "\n",
    "    gb_df.columns = gb_df.columns.droplevel(1)\n",
    "\n",
    "\n",
    "    # # # dummy_df\n",
    "    gb_df['label'] = np.where(gb_df['DirectEvidence'] == 'marker/mechanism',\n",
    "                                               gb_df['InferenceScore'] * -1,\n",
    "                                               gb_df['InferenceScore'])\n",
    "\n",
    "    return gb_df\n",
    "\n",
    "\n",
    "# park_train_df = get_disease_inference_df('Parkinsons Disease')\n",
    "# whole_network_df = get_disease_inference_df()\n",
    "\n",
    "# len(whole_network_df.columns)\n",
    "# print(whole_network_df['label'])\n",
    "# x = df.loc[df['DiseaseName'].str.contains('Parkinsons')]\n",
    "\n",
    "train_df = get_disease_inference_df()\n",
    "\n",
    "## USE THIS FOR PREDICTING THE INF SCORE\n",
    "# train_df['binary_label'] = np.where(train_df['DiseaseID'].isin(all_diseases),train_df['label'], 0)\n",
    "\n",
    "train_df['binary_label'] = np.where(train_df['DiseaseID'].isin(all_diseases),1, 0)\n",
    "\n",
    "train_df.to_csv('disease_inf.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6d86ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6840\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# disease_inf = pd.read_csv('disease_inf.csv')\n",
    "disease_inf[:5]\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# non_vector_columns = ['DiseaseID','ChemicalName','InferenceScore','DirectEvidence','DiseaseName','label']\n",
    "# vector_columns = [col for col in disease_inf if col not in non_vector_columns]\n",
    "\n",
    "# # self.vector_columns = vector_columns\n",
    "\n",
    "# X = disease_inf[vector_columns]\n",
    "\n",
    "# # X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "# from sklearn.decomposition import NMF\n",
    "# model = NMF(n_components=2, init='random', random_state=0)\n",
    "# W = model.fit_transform(X)\n",
    "# H = model.components_\n",
    "\n",
    "\n",
    "\n",
    "print(len(H[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1859e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gene_network(disease_name=None):\n",
    "#     \"\"\"\n",
    "#     this gets the gene network... doesnt look at direct evidence\n",
    "#     \"\"\"\n",
    "#     df = get_df('CTD_chemicals_diseases')\n",
    "\n",
    "#     gene_df = df.loc[df['DirectEvidence'].isnull()][['ChemicalName', 'DiseaseName', 'InferenceGeneSymbol', 'InferenceScore', 'DiseaseID']]\n",
    "\n",
    "#     dummy_df = pd.get_dummies(gene_df, prefix='', prefix_sep='',columns=['InferenceGeneSymbol'])\n",
    "#     gb_df = dummy_df.groupby(['DiseaseName', 'ChemicalName', 'DiseaseID']).agg({np.max}).reset_index()\n",
    "\n",
    "#     gb_df.columns = gb_df.columns.droplevel(1)\n",
    "    \n",
    "#     # dummy_df\n",
    "#     gb_df['label'] = gb_df['InferenceScore']\n",
    "\n",
    "#     return gb_df\n",
    "\n",
    "# train_df = get_gene_network()\n",
    "# train_df['label'] = np.where(train_df['DiseaseID'].isin(all_diseases),1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "240176e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52436\n",
      "472\n"
     ]
    }
   ],
   "source": [
    "# print(train_df[:5])\n",
    "\n",
    "print(len(train_df.loc[train_df['binary_label'] == 0]))\n",
    "print(len(train_df.loc[train_df['binary_label'] == 1]))\n",
    "\n",
    "# train_df['label'] = np.where(train_df['DiseaseID'].isin(all_diseases),1, 0)\n",
    "# print(train_df.groupby('label').size())\n",
    "# train_df = train_df.sample(frac=1)\n",
    "# train_df[train_df['DiseaseName'] == 'Parkinson Disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99612f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_label\n",
      "0    52436\n",
      "1      472\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = train_df.groupby(['binary_label']).size()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# park_df = train_df.loc[train_df['DiseaseName'] == 'Parkinson Disease']\n",
    "# train_df = park_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d455c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8569894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A  A1BG  A2M  AAAS  AADAC  AADAT  AASS  ABAT  ABCA1  ABCA12  ...  \\\n",
      "0      0     0    0     0      0      0     0     0      0       0  ...   \n",
      "1      0     0    0     0      0      0     0     0      0       0  ...   \n",
      "2      0     0    0     0      0      0     0     0      0       0  ...   \n",
      "3      0     0    0     0      0      0     0     0      0       0  ...   \n",
      "4      0     0    0     0      0      0     0     0      0       0  ...   \n",
      "...   ..   ...  ...   ...    ...    ...   ...   ...    ...     ...  ...   \n",
      "52903  0     0    0     0      0      0     0     0      0       0  ...   \n",
      "52904  0     0    0     0      0      0     0     0      0       0  ...   \n",
      "52905  0     0    0     0      0      0     0     0      0       0  ...   \n",
      "52906  0     0    0     0      0      0     0     0      0       0  ...   \n",
      "52907  0     0    0     0      0      0     0     0      0       0  ...   \n",
      "\n",
      "       ZPBP2  ZSCAN22  ZSCAN31  ZSWIM5  ZSWIM9  ZW10  ZWILCH  ZWINT  ZYX  \\\n",
      "0          0        0        0       0       0     0       0      0    0   \n",
      "1          0        0        0       0       0     0       0      0    0   \n",
      "2          0        0        0       0       0     0       0      0    0   \n",
      "3          0        0        0       0       0     0       0      0    0   \n",
      "4          0        0        0       0       0     0       0      0    0   \n",
      "...      ...      ...      ...     ...     ...   ...     ...    ...  ...   \n",
      "52903      0        0        0       0       0     0       0      0    0   \n",
      "52904      0        0        0       0       0     0       0      0    0   \n",
      "52905      0        0        0       0       0     0       0      0    0   \n",
      "52906      0        0        0       0       0     0       0      0    0   \n",
      "52907      0        0        0       0       0     0       0      0    0   \n",
      "\n",
      "       binary_label  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n",
      "...             ...  \n",
      "52903             0  \n",
      "52904             0  \n",
      "52905             0  \n",
      "52906             0  \n",
      "52907             0  \n",
      "\n",
      "[52908 rows x 6839 columns]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "class Algo:\n",
    "    def __init__(self, df, vector_size=50, model_type='DNN', plot=False, lstm_units=10,\\\n",
    "                 stop_early=True, verbose=1, epochs=EPOCHS, dropout_pct=.02):\n",
    "        self.df = df\n",
    "        self.vector_size = vector_size\n",
    "        self.model_type = model_type\n",
    "#         self.loss_fn = 'mean_squared_error'       \n",
    "        self.lstm_units = lstm_units\n",
    "        self.plot = plot\n",
    "        self.stop_early = stop_early\n",
    "        self.experiment_name = '{0} - vec: {1}'.format(model_type, vector_size)\n",
    "        self.dropout_pct = dropout_pct\n",
    "        self.verbose=verbose\n",
    "        self.epochs = EPOCHS\n",
    "\n",
    "    def vectorize(self):\n",
    "        non_vector_columns = ['DiseaseID','ChemicalName','InferenceScore','DirectEvidence','DiseaseName','label']\n",
    "\n",
    "        vector_columns = [col for col in self.df.columns if col not in non_vector_columns]\n",
    "        self.vector_columns = vector_columns\n",
    "\n",
    "        gene_vectors = self.df[vector_columns]\n",
    "#         pca = PCA(n_components=10)\n",
    "#         principalComponents = pca.fit_transform(gene_vectors)       \n",
    "#         return principalComponents\n",
    "        return gene_vectors\n",
    "    \n",
    "    # def get_model(self, input_len, data_shape):\n",
    "    def get_model(self, vectors):\n",
    "        \n",
    "        node_count = 8\n",
    "        output_label_len = 1\n",
    "        self.model = Sequential([\n",
    "            Dense(input_shape=[len(vectors[0])], units = node_count, activation = tf.nn.relu),\n",
    "            Dense(name = \"output_layer\", units = output_label_len, activation = tf.nn.softmax)\n",
    "        ])\n",
    "        # keras.utils.plot_model(self.model, \"mnist_model.png\", show_shapes=True)\n",
    "        self.model.compile(optimizer='rmsprop',           \n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "#         normalizer = Normalization(input_shape=[len(vectors[0]),], axis=None)\n",
    "#         normalizer.adapt(vectors)\n",
    "# #         [None, ]\n",
    "#         model = Sequential([\n",
    "#             normalizer,\n",
    "#             Dense(units=1)\n",
    "#         ])\n",
    "#         model.compile(\n",
    "#             optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "#             #loss='binary_crossentropy' #. mean_absolute_error\n",
    "#             loss='mean_absolute_error'\n",
    "#         )\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def train(self, train_vectors, train_labels):\n",
    "        \n",
    "        callbacks = [] \n",
    "#         if self.stop_early:\n",
    "#             callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2))\n",
    "\n",
    "        input_len = len(train_vectors[0])\n",
    "        outputlen = 1\n",
    "    \n",
    "        # model = self.get_model(input_len, train_vectors.shape)\n",
    "        model = self.get_model(train_vectors)\n",
    "\n",
    "        history = model.fit(train_vectors, \n",
    "                        train_labels, \n",
    "                        batch_size=8, \n",
    "                        epochs=self.epochs, \n",
    "                        verbose=self.verbose, \n",
    "                        validation_split=0.3, \n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "        return model, history\n",
    "    \n",
    "    def evaluate(self, model, test_vectors, test_labels):\n",
    "        res = model.evaluate(test_vectors, test_labels, verbose=self.verbose)\n",
    "        return res\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "p\n",
    "        \n",
    "    def main(self):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        v = self.vectorize()\n",
    "\n",
    "        print(v)\n",
    "        label_vecs = self.df['binary_label']\n",
    "        \n",
    "        train_vectors, test_vectors, train_labels, test_labels = train_test_split(\n",
    "              np.array(v), label_vecs, test_size=0.1)\n",
    "        \n",
    "        model, history = self.train(train_vectors, train_labels)\n",
    "        # test_loss, _ = self.evaluate(model, test_vectors, test_labels)\n",
    "        self.plot_loss(history)\n",
    "#         predictions = self.predict(model, test_vectors, test_labels)\n",
    "        return model\n",
    "#        return predictions, results, model\n",
    "        \n",
    "#         train_loss = history.history['loss'][-1]\n",
    "#         validation_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "#         end_time = datetime.datetime.now()\n",
    "#         task_duration = (end_time-start_time).total_seconds()\n",
    "#         results = {\n",
    "#             'experiment_name': self.experiment_name,\n",
    "#             'task_duration': task_duration,\n",
    "#             'test_loss': test_loss,\n",
    "#             'train_loss': train_loss,\n",
    "#             'validation_loss': validation_loss,\n",
    "#             'epochs_completed': len(history.history['accuracy']),\n",
    "#             'epochs': EPOCHS,\n",
    "#             'stop_early': self.stop_early,\n",
    "#             'vector_size': self.vector_size,\n",
    "#             'dropout_pct': self.dropout_pct,\n",
    "#             'lstm_units': self.lstm_units,\n",
    "#             'model_type':self.model_type\n",
    "#         }\n",
    "#         if self.plot:\n",
    "#             self.plot_loss(history)\n",
    "        \n",
    "#         return predictions, results, model\n",
    "\n",
    "    def predict(self, model, test_vectors, test_labels):\n",
    "\n",
    "        preds = model.predict(test_vectors).flatten()\n",
    "\n",
    "        test_labels['predictions'] = preds\n",
    "        final = self.df.merge(test_labels, left_index=True, right_index=True)\n",
    "        return final\n",
    "\n",
    "\n",
    "\n",
    "a = Algo(train_df, vector_size=50, model_type = 'DNN', plot=True)\n",
    "v = a.vectorize()\n",
    "# print(a.vector_columns)\n",
    "print(v)\n",
    "# test_predictions, results, trained_model  = a.main()\n",
    "# trained_model  = a.main()\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:-5]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6524f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['preds'] = trained_model.predict(train_df[a.vector_columns])\n",
    "# train_df.sort_values('preds', ascending=False)[:5]\n",
    "train_df.sort_values('preds', ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd388b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_disease_df = train_df.loc[train_df['DiseaseName'] == 'Parkinson Disease']\n",
    "park_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa35ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_df = get_df('CTD_chem_gene_ixns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_df[:5]\n",
    "# # dopamine_df = inp_df.loc[inp_df['ChemicalName'] == 'Dopamine'][['ChemicalName', 'GeneSymbol']]\n",
    "\n",
    "# dopamine_df = inp_df[['ChemicalName', 'GeneSymbol']]\n",
    "\n",
    "# gb_df = dopamine_df.groupby(['ChemicalName', 'GeneSymbol']).size().reset_index()\n",
    "# gb_df.columns = ['ChemicalName','GeneSymbol', 'InteractionCount']\n",
    "\n",
    "# # gb_df.columns = gb_df.columns.droplevel(1)\n",
    "# gb_df[:5]\n",
    "\n",
    "# dummy_df = pd.get_dummies(gb_df, prefix='', prefix_sep='',columns=['GeneSymbol'])\n",
    "\n",
    "# # dummy_df.sort_values('InteractionCount', ascending=False)[:5]\n",
    "\n",
    "# gb_df = dummy_df.groupby(['ChemicalName']).agg({np.max}).reset_index()\n",
    "# gb_df.columns = gb_df.columns.droplevel(1)\n",
    "\n",
    "# gb_df\n",
    "\n",
    "# trained_model.predict(gb_df[a.vector_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f943802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
